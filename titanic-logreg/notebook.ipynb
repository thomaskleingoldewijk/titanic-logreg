{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1265e2bf",
   "metadata": {},
   "source": [
    "### 0. Summary \n",
    "\n",
    "In this notebook, we will use the Titanic dataset. This data consists of demographic and traveling information for 891 of the Titanic's passengers.\n",
    "The goal is to predict which of these passengers survived.\n",
    "\n",
    "To predict this, we will follow four steps: \n",
    "1. First, we will clean up the data, for example by replacing missing values and by scaling the features \n",
    "2. Secondly, we will perform gradient descent to learn how probable it is that different people on the Titanic survived (e.g. based on age, gender, etc.)\n",
    "3. Thirdly, we will determine the accuracy of these predictions. \n",
    "4. Fourthly, we will analyze the patterns in the data and visualize these patterns. \n",
    "\n",
    "To start with the conclusions: \n",
    "- More people died than survived, and women have a much higher survival rate than men.\n",
    "Possibly due to the fact that they were put into life boats first. \n",
    "- People who resided in the third class had a very low survival rate, possibly due to the position of the third-class cabins on the Titanic. \n",
    "- Younger people have a higher survival probability than older people, possibly due to their improved physical fitness. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91d657",
   "metadata": {},
   "source": [
    "### 1. Cleaning the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4abfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('titanic.csv')\n",
    "\n",
    "display(data.head())\n",
    "data.info()\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll remove features from the data that we won't use \n",
    "clean_data = data.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\n",
    "\n",
    "clean_data.info()\n",
    "\n",
    "# Since the cabin feature has many missing values, we will drop them for now \n",
    "clean_data = clean_data.drop(['Cabin'], axis=1)\n",
    "\n",
    "clean_data.info()\n",
    "\n",
    "# Since there are only two missing values for 'Embarked', we'll drop these \n",
    "clean_data = clean_data.dropna(subset=['Embarked'])\n",
    "\n",
    "clean_data.info()\n",
    "\n",
    "# We'll replace missing age values by the mean age \n",
    "mean = clean_data['Age'].mean()\n",
    "clean_data['Age'] = clean_data['Age'].fillna(mean)\n",
    "\n",
    "clean_data.info()\n",
    "display(clean_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a6a24",
   "metadata": {},
   "source": [
    "We will also need to transform the categorical data ('Sex' and 'Embarked') into numerical data that our logistical model can work with. We will transform our categorical data using one-hot encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new columns for the male and female one-hot encoding\n",
    "sex = pd.get_dummies(clean_data['Sex'], dtype=int)\n",
    "\n",
    "# Create three new columns for port of embarkment one-hot encoding\n",
    "embark = pd.get_dummies(clean_data['Embarked'], dtype=int)\n",
    "\n",
    "# We no longer need these columns, as we will replace them soon\n",
    "clean_data = clean_data.drop(['Sex','Embarked'], axis=1)\n",
    "\n",
    "# Create the new dataframe with the one-hot columns we created from the categories in Sex and Embarked\n",
    "clean_data = pd.concat([clean_data, sex, embark], axis=1)\n",
    "\n",
    "clean_data.info()\n",
    "display(clean_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f458fc8",
   "metadata": {},
   "source": [
    "Now we will normalize the data to prevent any feature dominating the rest.\n",
    "Also, we'll split the data into training and testing data, so that we can determine the accuracy of our model later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "numerical = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "clean_data[numerical] = clean_data[numerical].apply(zscore)\n",
    "\n",
    "display(clean_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86864eb",
   "metadata": {},
   "source": [
    "### 2. Making the predictions\n",
    "Now that we have cleaned the data, we will train our logistic model using gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abe38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into target \"y\" and input \"X\"\n",
    "y_df = clean_data['Survived']\n",
    "X_df = clean_data.drop('Survived', axis=1)\n",
    "\n",
    "# Convert the Pandas DataFrames to Numpy ndarrays\n",
    "X_np = X_df.to_numpy()\n",
    "y_np = np.expand_dims(y_df.to_numpy(), axis=1)\n",
    "\n",
    "#Split the data into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, train_size=0.7, random_state=1265599650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9243fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_func(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "z_values = np.linspace(-5, 5, 100)\n",
    "g_values = logistic_func(z_values)\n",
    "\n",
    "def logistic_model(X, w, b):\n",
    "    \"\"\"\n",
    "    Compute the logistic regression hypothesis for input matrix X.\n",
    "\n",
    "    Parameters:\n",
    "    - X: input features, shape (n_samples, n_features)\n",
    "    - w: weights vector, shape (n_features,)\n",
    "    - b: scalar bias\n",
    "\n",
    "    Returns:\n",
    "    - probabilities: shape (n_samples,)\n",
    "    \"\"\"\n",
    "    z_scores = np.matmul(X, w) + b \n",
    "    return logistic_func(z_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost(w, b, X, y):\n",
    "    \"\"\"\n",
    "    Compute the binary cross-entropy cost for logistic regression.\n",
    "\n",
    "    Parameters:\n",
    "    - w (ndarray): Weight vector of shape (n_features,)\n",
    "    - b (float): Scalar bias term\n",
    "    - X (ndarray): Input feature matrix of shape (m_samples, n_features)\n",
    "    - y (ndarray): True labels of shape (m_samples,), with values 0 or 1\n",
    "\n",
    "    Returns:\n",
    "    - cost (float): Average binary cross-entropy loss over all samples\n",
    "    \"\"\"\n",
    "    # Step 1: Compute predicted probabilities using logistic model\n",
    "    y_hat = logistic_model(X, w, b)\n",
    "\n",
    "    # Step 2: Prevent numerical issues by avoiding log(0)\n",
    "    y_hat = np.clip(y_hat, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    # Step 3: Compute binary cross-entropy loss for each sample\n",
    "    loss_per_sample = y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)\n",
    "\n",
    "    # Step 4: Take mean over all samples and flip the sign to return positive cost\n",
    "    return -np.mean(loss_per_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc61234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_gradient(w, b, X, y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the logistic cost function with respect to the bias term b.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    w : numpy.ndarray\n",
    "        The weight vector.\n",
    "    b : float\n",
    "        The bias term.\n",
    "    X : numpy.ndarray\n",
    "        The input feature matrix.\n",
    "    y : numpy.ndarray\n",
    "        The vector of true labels.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The average gradient of the cost function with respect to b.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    # Use logistic_model to compute predictions\n",
    "    total_error = np.sum(logistic_model(X, w, b) - y)\n",
    "    return total_error / m\n",
    "\n",
    "def w_gradient(w, b, X, y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the logistic cost function with respect to the weight vector w.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    w : numpy.ndarray\n",
    "        The weight vector.\n",
    "    b : float\n",
    "        The bias term.\n",
    "    X : numpy.ndarray\n",
    "        The input feature matrix.\n",
    "    y : numpy.ndarray\n",
    "        The vector of true labels.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        The gradient of the cost function with respect to w.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    # Use logistic_model to compute predictions for logistic regression\n",
    "    y_hat = logistic_model(X, w, b)\n",
    "    errors = y_hat - y  # error vector\n",
    "    # Compute gradient: dot product of features and error vector, scaled by m\n",
    "    grad = np.matmul(X.T, errors) / m\n",
    "    return grad\n",
    "\n",
    "def gradient_descent(X, y, w, b, alpha, thres=1e-6):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the logistic regression cost function.\n",
    "\n",
    "    This function updates the weight vector w and bias b iteratively using\n",
    "    their respective gradients until the change in cost is below the specified threshold\n",
    "    or divergence is detected.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        The input feature matrix.\n",
    "    y : numpy.ndarray\n",
    "        The vector of true labels.\n",
    "    w : numpy.ndarray\n",
    "        The initial weight vector.\n",
    "    b : float\n",
    "        The initial bias term.\n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    thres : float, optional\n",
    "        The convergence threshold for the change in cost (default is 1e-6).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple (w, b) containing the optimized weight vector and bias term.\n",
    "    \"\"\"\n",
    "    cost_delta = logistic_cost(w, b, X, y)\n",
    "    \n",
    "    while cost_delta > thres and cost_delta > 0:\n",
    "        cost_prior = logistic_cost(w, b, X, y)\n",
    "        \n",
    "        # Compute gradients concurrently\n",
    "        grad_w = w_gradient(w, b, X, y)\n",
    "        grad_b = b_gradient(w, b, X, y)\n",
    "        \n",
    "        # Update parameters simultaneously\n",
    "        w = w - alpha * grad_w\n",
    "        b = b - alpha * grad_b\n",
    "        \n",
    "        cost_posterior = logistic_cost(w, b, X, y)\n",
    "        cost_delta = cost_prior - cost_posterior\n",
    "        \n",
    "        if cost_delta < 0:\n",
    "            print(f\"Cost increased by {cost_delta}; stopping to avoid divergence.\")\n",
    "            break\n",
    "            \n",
    "    return w, b\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.zeros((X_train.shape[1], 1))\n",
    "b = 0\n",
    "\n",
    "# Run gradient descent\n",
    "w_hat, b_hat = gradient_descent(X_train, y_train, w, b, alpha=1e-4)\n",
    "\n",
    "print('w:', w_hat)\n",
    "print('b:', b_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_learning_curves(X_train, y_train, X_test, y_test, w, b, alpha, thres=1e-6):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for logistic regression while tracking and plotting the learning curves.\n",
    "\n",
    "    This function implements gradient descent to minimize the logistic cost function on the training data,\n",
    "    while also storing the training and testing cost at every iteration. After convergence (or divergence),\n",
    "    it plots the learning curves (training and testing cost vs. iteration) and returns the final parameters.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : numpy.ndarray\n",
    "        Training feature matrix.\n",
    "    y_train : numpy.ndarray\n",
    "        Training labels.\n",
    "    X_test : numpy.ndarray\n",
    "        Testing feature matrix.\n",
    "    y_test : numpy.ndarray\n",
    "        Testing labels.\n",
    "    w : numpy.ndarray\n",
    "        Initial weight vector.\n",
    "    b : float\n",
    "        Initial bias.\n",
    "    alpha : float\n",
    "        Learning rate.\n",
    "    thres : float, optional\n",
    "        Convergence threshold for the change in cost (default is 1e-6).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        A tuple (w, b) containing the optimized weight vector and bias.\n",
    "    \"\"\"\n",
    "    cost_delta = logistic_cost(w, b, X_train, y_train)\n",
    "    \n",
    "    # Initialize lists to store training and testing costs at each iteration\n",
    "    training_costs = []\n",
    "    test_costs = []\n",
    "    \n",
    "    # Run gradient descent until the change in cost is below the threshold or divergence is detected\n",
    "    while cost_delta > thres and cost_delta > 0:\n",
    "        cost_prior = logistic_cost(w, b, X_train, y_train)\n",
    "        \n",
    "        # Store current training and testing costs\n",
    "        training_costs.append(cost_prior)\n",
    "        test_costs.append(logistic_cost(w, b, X_test, y_test))\n",
    "        \n",
    "        # Compute gradients concurrently\n",
    "        grad_w = w_gradient(w, b, X_train, y_train)\n",
    "        grad_b = b_gradient(w, b, X_train, y_train)\n",
    "        \n",
    "        # Update parameters simultaneously\n",
    "        w = w - alpha * grad_w\n",
    "        b = b - alpha * grad_b\n",
    "        \n",
    "        cost_posterior = logistic_cost(w, b, X_train, y_train)\n",
    "        cost_delta = cost_prior - cost_posterior\n",
    "        \n",
    "        if cost_delta < 0:\n",
    "            print(f\"Cost increased by {cost_delta}; stopping to avoid divergence.\")\n",
    "            break\n",
    "    \n",
    "    # After gradient descent, compute the iteration numbers based on the cost lists\n",
    "    number_of_iterations = list(range(len(training_costs)))\n",
    "    \n",
    "    # Plot the learning curves for training and testing costs\n",
    "    plt.plot(number_of_iterations, training_costs, color='blue', label='Training Cost')\n",
    "    plt.plot(number_of_iterations, test_costs, color='red', label='Testing Cost')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# Example initialization and function call:\n",
    "w = np.zeros((X_train.shape[1], 1))\n",
    "b = 0\n",
    "\n",
    "w_hat, b_hat = gradient_descent_learning_curves(X_train, y_train, X_test, y_test, w, b, alpha=4)\n",
    "\n",
    "print('w:', w_hat)\n",
    "print('b:', b_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Predict binary outcomes using a logistic regression model.\n",
    "\n",
    "    This function computes predicted probabilities for each sample using the logistic_model,\n",
    "    then converts these probabilities into binary predictions (0 or 1) using a decision boundary\n",
    "    of 0.5. It uses boolean masking to update the probabilities in place:\n",
    "    - Values >= 0.5 are set to 1.\n",
    "    - Values < 0.5 are set to 0.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        The input feature matrix, where each row corresponds to a sample.\n",
    "    w : numpy.ndarray\n",
    "        The weight vector for the logistic regression model.\n",
    "    b : float\n",
    "        The bias term for the logistic regression model.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        An array of predictions (0 or 1) for each sample in X.\n",
    "    \"\"\"\n",
    "    probabilities = logistic_model(X, w, b)\n",
    "    mask = probabilities >= 0.5\n",
    "    probabilities[mask] = 1  \n",
    "    probabilities[~mask] = 0\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "# Example usage:\n",
    "predictions_train = predict(X_train, w_hat, b_hat)\n",
    "predictions_test = predict(X_test, w_hat, b_hat)\n",
    "print(predictions_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cdd6f6",
   "metadata": {},
   "source": [
    "### 3. Computing accuracy\n",
    "Now that we made our predictions, we will calculate how accurate these predictions are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(predictions, y):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy based on the condition that a prediction is correct\n",
    "    if the sum of prediction and true label is 0 (both 0) or 2 (both 1).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : numpy.ndarray\n",
    "        Vector of predicted labels (0 or 1).\n",
    "    y : numpy.ndarray\n",
    "        Vector of true labels (0 or 1).\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        Accuracy as a ratio between 0 and 1.\n",
    "    \"\"\"\n",
    "    total = predictions + y\n",
    "    mask = (total == 0) | (total == 2)\n",
    "    correct = np.sum(mask)\n",
    "    return correct / predictions.shape[0]\n",
    "\n",
    "    \n",
    "\n",
    "print(f\"Training set accuracy: {calc_accuracy(predictions_train, y_train)}\")\n",
    "print(f\"Testing set accuracy: {calc_accuracy(predictions_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a4574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[138  23]\n",
      " [ 36  70]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>w_own</th>\n",
       "      <th>w_sklearn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>male</td>\n",
       "      <td>-1.471451</td>\n",
       "      <td>-1.423554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pclass</td>\n",
       "      <td>-0.897371</td>\n",
       "      <td>-0.870681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Age</td>\n",
       "      <td>-0.437385</td>\n",
       "      <td>-0.423669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SibSp</td>\n",
       "      <td>-0.344705</td>\n",
       "      <td>-0.331368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>S</td>\n",
       "      <td>-0.328266</td>\n",
       "      <td>-0.321896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parch</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.007548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Q</td>\n",
       "      <td>0.107903</td>\n",
       "      <td>0.118510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fare</td>\n",
       "      <td>0.115874</td>\n",
       "      <td>0.120624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C</td>\n",
       "      <td>0.147769</td>\n",
       "      <td>0.142152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>female</td>\n",
       "      <td>1.398857</td>\n",
       "      <td>1.362320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature     w_own  w_sklearn\n",
       "6    male -1.471451  -1.423554\n",
       "0  Pclass -0.897371  -0.870681\n",
       "1     Age -0.437385  -0.423669\n",
       "2   SibSp -0.344705  -0.331368\n",
       "9       S -0.328266  -0.321896\n",
       "3   Parch  0.003100   0.007548\n",
       "8       Q  0.107903   0.118510\n",
       "4    Fare  0.115874   0.120624\n",
       "7       C  0.147769   0.142152\n",
       "5  female  1.398857   1.362320"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def confusion_matrix(predictions, y):\n",
    "    \"\"\"\n",
    "    Compute the 2×2 confusion matrix for binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : numpy.ndarray\n",
    "        Vector of predicted labels (0 or 1).\n",
    "    y : numpy.ndarray\n",
    "        Vector of true labels (0 or 1).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        A 2×2 confusion matrix in the format:\n",
    "        [[TN, FP],[FN, TP]]\n",
    "    \"\"\"\n",
    "    stacked = np.stack((predictions, y), axis=1)\n",
    "    \n",
    "    # Unpack columns: pred = stacked[:, 0], truth = stacked[:, 1]\n",
    "    TP = np.sum((stacked[:, 0] == 1) & (stacked[:, 1] == 1))  # predicted 1, actual 1\n",
    "    FP = np.sum((stacked[:, 0] == 1) & (stacked[:, 1] == 0))  # predicted 1, actual 0\n",
    "    FN = np.sum((stacked[:, 0] == 0) & (stacked[:, 1] == 1))  # predicted 0, actual 1\n",
    "    TN = np.sum((stacked[:, 0] == 0) & (stacked[:, 1] == 0))  # predicted 0, actual 0\n",
    "    \n",
    "    return np.array([[TN, FP],\n",
    "                     [FN, TP]])\n",
    "\n",
    "\n",
    "print(confusion_matrix(predictions_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "750ccc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7035175879396985\n",
      "ROC-AUC: 0.8327669049572248\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, RocCurveDisplay\n",
    "\n",
    "proba_test = logistic_model(X_test, w_hat, b_hat).ravel()\n",
    "print(\"F1:\", f1_score(y_test.ravel(), predictions_test.ravel()))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test.ravel(), proba_test))\n",
    "RocCurveDisplay.from_predictions(y_test.ravel(), proba_test)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c6340",
   "metadata": {},
   "source": [
    "We just implemented our own version of logistic regression. \n",
    "We can compare our results to using the Sklearn predefined logistic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# 1. Train sklearn LogisticRegression \n",
    "clf = LogisticRegression(max_iter=1000)  \n",
    "clf.fit(X_train, y_train.ravel())  \n",
    "\n",
    "# 2. Make predictions\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# 3. Calculate accuracy\n",
    "print(\"Sklearn LogisticRegression:\")\n",
    "print(\" - Training accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\" - Testing accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "\n",
    "# 4. Confusion matrix for testset\n",
    "print(\"Confusion matrix (test):\")\n",
    "print(confusion_matrix(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92cba2",
   "metadata": {},
   "source": [
    "### 4. Analyzing and visualing patterns in the data\n",
    "Now we have established the accuracy of our model (the sklearn accuracy is identical to the accuracy of our own model), we can start to determine which features are predictive of survival-rates and draw our conclusions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca96f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=X_df.columns, y=np.squeeze(w_hat))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07cebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Survived',data=data)\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='Survived',hue='Sex',data=data)\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='Survived',hue='Pclass',data=data)\n",
    "plt.show()\n",
    "\n",
    "sns.barplot(x='Survived', y='Age', data=data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36029e4",
   "metadata": {},
   "source": [
    "#### Creating hypotheses for the data\n",
    "\n",
    "For each of the 3 features plotted above, we compare their individual impact on the survival chance of a passenger. For every feature, we explain this feature has a positive or negative effect on the survival chance of a passenger. And we provide an hypothesis on why this feature is likely to affect survival. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7a24e",
   "metadata": {},
   "source": [
    "**Q1. Gender of the passenger**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77145b69",
   "metadata": {},
   "source": [
    "Logistic Regression Finding:\n",
    "\n",
    "- “male” has a strongly negative weight.\n",
    "\n",
    "- “female” has a strongly positive weight.\n",
    "Interpreted together, the model suggests being female substantially increases the predicted probability of survival compared to being male.\n",
    "\n",
    "Possible Reasons:\n",
    "\n",
    "- Evacuation Priorities: Historically (and famously in the Titanic’s case), the phrase “women and children first” often guided rescue protocols.\n",
    "\n",
    "- Physical/Behavioral Factors: Women may have been helped first by other passengers or crew.\n",
    "\n",
    "- Social Norms: Cultural expectations of the era could have led to women being escorted to lifeboats more readily.\n",
    "\n",
    "Connecting to the Weight:\n",
    "\n",
    "- A positive weight for “female” means that, after standardizing, higher values in this feature (i.e., being female = 1) push the log-odds of survival up.\n",
    "\n",
    "- A negative weight for “male” means that, for passengers identified as male, the predicted probability of survival goes down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac69b3",
   "metadata": {},
   "source": [
    "**Q2. Class of the passenger's cabin**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aedb515",
   "metadata": {},
   "source": [
    "The analysis shows that third-class passengers had markedly lower survival rates. The corresponding weight for third-class status is strongly negative.\n",
    "    \n",
    "Hypothesis: This might be because third-class cabins were located further from the exits or lifeboats, or passengers in this class might have faced more barriers during evacuation (such as less guidance or physical obstacles).\n",
    "\n",
    "Because passenger class is ordinal, a single negative weight for Pclass indicates that each “step” upward in class number (from 1 to 2, 2 to 3) lowers the predicted survival probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b191475",
   "metadata": {},
   "source": [
    "**Q3. Age of the passenger**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62f59c",
   "metadata": {},
   "source": [
    "A negative weight for “Age” means older passengers have lower predicted survival probability. In the Titanic dataset, the evidence points to younger passengers faring better.\n",
    "\n",
    "Possible Reasons:\n",
    "\n",
    "- Physical Agility: Younger individuals may have been better able to move quickly through crowded corridors or climb obstacles to reach safety.\n",
    "- Evacuation Priorities: Children were sometimes given priority in lifeboats, tying back to “women and children first.”\n",
    "- Health and Stamina: Younger passengers might withstand cold temperatures and the stress of evacuation better than older passengers.\n",
    "\n",
    "Connecting to the Weight:\n",
    "\n",
    "- After z-score scaling, if “Age” is above the mean, the model’s negative weight lowers the survival probability.\n",
    "- Being younger than average effectively “raises” your predicted survival probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
